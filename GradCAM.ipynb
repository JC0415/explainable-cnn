{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a9affec-c801-4a44-ade5-c9118042a39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashutosh1919/miniforge3/envs/cnn_xai/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import pickle\n",
    "import os.path as osp\n",
    "\n",
    "import cv2\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "\n",
    "from grad_cam import (\n",
    "    BackPropagation,\n",
    "    Deconvnet,\n",
    "    GradCAM,\n",
    "    GuidedBackPropagation,\n",
    "    occlusion_sensitivity,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "349796ee-6bb1-4d20-b651-2851309241d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device(cuda):\n",
    "    cuda = cuda and torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "    if cuda:\n",
    "        current_device = torch.cuda.current_device()\n",
    "        print(\"Device:\", torch.cuda.get_device_name(current_device))\n",
    "    else:\n",
    "        print(\"Device: CPU\")\n",
    "    return device\n",
    "\n",
    "\n",
    "def load_images(image_paths):\n",
    "    images = []\n",
    "    raw_images = []\n",
    "    print(\"Images:\")\n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        print(\"\\t#{}: {}\".format(i, image_path))\n",
    "        image, raw_image = preprocess(image_path)\n",
    "        images.append(image)\n",
    "        raw_images.append(raw_image)\n",
    "    return images, raw_images\n",
    "\n",
    "\n",
    "def get_classtable():\n",
    "    classes = []\n",
    "    for index in range(1000):\n",
    "        line = imagenet_class_labels[index]\n",
    "        line = line.split(\", \", 1)[0].replace(\" \", \"_\")\n",
    "        classes.append(line)\n",
    "    # with open(\"./data/synset_words.txt\") as lines:\n",
    "    #     for line in lines:\n",
    "    #         line = line.strip().split(\" \", 1)[1]\n",
    "    #         line = line.split(\", \", 1)[0].replace(\" \", \"_\")\n",
    "    #         classes.append(line)\n",
    "    return classes\n",
    "\n",
    "\n",
    "def preprocess(image_path):\n",
    "    raw_image = cv2.imread(image_path)\n",
    "    raw_image = cv2.resize(raw_image, (224,) * 2)\n",
    "    image = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )(raw_image[..., ::-1].copy())\n",
    "    return image, raw_image\n",
    "\n",
    "\n",
    "def save_gradient(filename, gradient):\n",
    "    gradient = gradient.cpu().numpy().transpose(1, 2, 0)\n",
    "    gradient -= gradient.min()\n",
    "    gradient /= gradient.max()\n",
    "    gradient *= 255.0\n",
    "    cv2.imwrite(filename, np.uint8(gradient))\n",
    "\n",
    "\n",
    "def save_gradcam(filename, gcam, raw_image, paper_cmap=False):\n",
    "    gcam = gcam.cpu().numpy()\n",
    "    cmap = cm.jet_r(gcam)[..., :3] * 255.0\n",
    "    if paper_cmap:\n",
    "        alpha = gcam[..., None]\n",
    "        gcam = alpha * cmap + (1 - alpha) * raw_image\n",
    "    else:\n",
    "        gcam = (cmap.astype(np.float) + raw_image.astype(np.float)) / 2\n",
    "    cv2.imwrite(filename, np.uint8(gcam))\n",
    "\n",
    "\n",
    "def save_sensitivity(filename, maps):\n",
    "    maps = maps.cpu().numpy()\n",
    "    scale = max(maps[maps > 0].max(), -maps[maps <= 0].min())\n",
    "    maps = maps / scale * 0.5\n",
    "    maps += 0.5\n",
    "    maps = cm.bwr_r(maps)[..., :3]\n",
    "    maps = np.uint8(maps * 255.0)\n",
    "    maps = cv2.resize(maps, (224, 224), interpolation=cv2.INTER_NEAREST)\n",
    "    cv2.imwrite(filename, maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20095072-18f3-4057-9ca0-1e8e13ee3320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchvision models\n",
    "model_names = sorted(\n",
    "    name\n",
    "    for name in models.__dict__\n",
    "    if name.islower() and not name.startswith(\"__\") and callable(models.__dict__[name])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfd7d82d-7334-49be-a627-de944efc4b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.resnet18()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0151a953-4c91-4417-afc7-aea54ff5750c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/imagenet_class_labels.pkl\", \"rb\") as label_file:\n",
    "    imagenet_class_labels = pickle.load(label_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a84544c-4fa6-4a05-8078-fbd76035f42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DifferentVisualDemo(image_paths, target_layer, arch, topk, output_dir, cuda):\n",
    "    \"\"\"\n",
    "    Visualize model responses given multiple images.\n",
    "    \"\"\"\n",
    "    device = get_device(cuda)\n",
    "\n",
    "    # Synset words\n",
    "    classes = get_classtable()\n",
    "    # print(classes)\n",
    "\n",
    "    # Model from torchvision\n",
    "    model = models.__dict__[arch](pretrained=True)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Images\n",
    "    images, raw_images = load_images(image_paths)\n",
    "    images = torch.stack(images).to(device)\n",
    "    \n",
    "    \"\"\"\n",
    "    Common usage:\n",
    "    1. Wrap your model with visualization classes defined in grad_cam.py\n",
    "    2. Run forward() with images\n",
    "    3. Run backward() with a list of specific classes\n",
    "    4. Run generate() to export results\n",
    "    \"\"\"\n",
    "    \n",
    "    # =========================================================================\n",
    "    print(\"Vanilla Backpropagation:\")\n",
    "\n",
    "    bp = BackPropagation(model=model)\n",
    "    probs, ids = bp.forward(images)  # sorted\n",
    "    \n",
    "    for i in range(topk):\n",
    "        bp.backward(ids=ids[:, [i]])\n",
    "        gradients = bp.generate()\n",
    "\n",
    "        # Save results as image files\n",
    "        for j in range(len(images)):\n",
    "            print(\"\\t#{}: {} ({:.5f})\".format(j, classes[ids[j, i]], probs[j, i]))\n",
    "\n",
    "            save_gradient(\n",
    "                filename=osp.join(\n",
    "                    output_dir,\n",
    "                    \"{}-{}-vanilla-{}.png\".format(j, arch, classes[ids[j, i]]),\n",
    "                ),\n",
    "                gradient=gradients[j],\n",
    "            )\n",
    "    \n",
    "    # Remove all the hook function in the \"model\"\n",
    "    bp.remove_hook()\n",
    "    \n",
    "    # =========================================================================\n",
    "    print(\"Deconvolution:\")\n",
    "\n",
    "    deconv = Deconvnet(model=model)\n",
    "    _ = deconv.forward(images)\n",
    "\n",
    "    for i in range(topk):\n",
    "        deconv.backward(ids=ids[:, [i]])\n",
    "        gradients = deconv.generate()\n",
    "\n",
    "        for j in range(len(images)):\n",
    "            print(\"\\t#{}: {} ({:.5f})\".format(j, classes[ids[j, i]], probs[j, i]))\n",
    "\n",
    "            save_gradient(\n",
    "                filename=osp.join(\n",
    "                    output_dir,\n",
    "                    \"{}-{}-deconvnet-{}.png\".format(j, arch, classes[ids[j, i]]),\n",
    "                ),\n",
    "                gradient=gradients[j],\n",
    "            )\n",
    "\n",
    "    deconv.remove_hook()\n",
    "    \n",
    "    # =========================================================================\n",
    "    print(\"Grad-CAM/Guided Backpropagation/Guided Grad-CAM:\")\n",
    "\n",
    "    gcam = GradCAM(model=model)\n",
    "    _ = gcam.forward(images)\n",
    "\n",
    "    gbp = GuidedBackPropagation(model=model)\n",
    "    _ = gbp.forward(images)\n",
    "\n",
    "    for i in range(topk):\n",
    "        # Guided Backpropagation\n",
    "        gbp.backward(ids=ids[:, [i]])\n",
    "        gradients = gbp.generate()\n",
    "\n",
    "        # Grad-CAM\n",
    "        gcam.backward(ids=ids[:, [i]])\n",
    "        regions = gcam.generate(target_layer=target_layer)\n",
    "\n",
    "        for j in range(len(images)):\n",
    "            print(\"\\t#{}: {} ({:.5f})\".format(j, classes[ids[j, i]], probs[j, i]))\n",
    "\n",
    "            # Guided Backpropagation\n",
    "            save_gradient(\n",
    "                filename=osp.join(\n",
    "                    output_dir,\n",
    "                    \"{}-{}-guided-{}.png\".format(j, arch, classes[ids[j, i]]),\n",
    "                ),\n",
    "                gradient=gradients[j],\n",
    "            )\n",
    "\n",
    "            # Grad-CAM\n",
    "            save_gradcam(\n",
    "                filename=osp.join(\n",
    "                    output_dir,\n",
    "                    \"{}-{}-gradcam-{}-{}.png\".format(\n",
    "                        j, arch, target_layer, classes[ids[j, i]]\n",
    "                    ),\n",
    "                ),\n",
    "                gcam=regions[j, 0],\n",
    "                raw_image=raw_images[j],\n",
    "            )\n",
    "\n",
    "            # Guided Grad-CAM\n",
    "            save_gradient(\n",
    "                filename=osp.join(\n",
    "                    output_dir,\n",
    "                    \"{}-{}-guided_gradcam-{}-{}.png\".format(\n",
    "                        j, arch, target_layer, classes[ids[j, i]]\n",
    "                    ),\n",
    "                ),\n",
    "                gradient=torch.mul(regions, gradients)[j],\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f69563b-fb17-4334-bc9d-956f12f617a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: CPU\n",
      "Images:\n",
      "\t#0: ./data/images/tiger_shark.jpeg\n",
      "Vanilla Backpropagation:\n",
      "\t#0: tiger_shark (0.97557)\n",
      "Deconvolution:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashutosh1919/miniforge3/envs/cnn_xai/lib/python3.8/site-packages/torch/nn/modules/module.py:1025: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t#0: tiger_shark (0.97557)\n",
      "Grad-CAM/Guided Backpropagation/Guided Grad-CAM:\n",
      "\t#0: tiger_shark (0.97557)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/pm6_lt111h92v74z5vt3_vhh0000gn/T/ipykernel_95377/2648377605.py:65: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  gcam = (cmap.astype(np.float) + raw_image.astype(np.float)) / 2\n"
     ]
    }
   ],
   "source": [
    "DifferentVisualDemo([\"./data/images/tiger_shark.jpeg\"], \"layer4\", \"resnet18\", 1, \"./data/outputs/\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e8b38fd-e72a-4674-88be-a03f71e8b015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LayersDemo(image_paths, target_class, output_dir, cuda):\n",
    "    device = get_device(cuda)\n",
    "\n",
    "    # Synset words\n",
    "    classes = get_classtable()\n",
    "    # print(classes)\n",
    "\n",
    "    # Model from torchvision\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # The four residual layers\n",
    "    target_layers = [\"relu\", \"layer1\", \"layer2\", \"layer3\", \"layer4\"]\n",
    "\n",
    "    # Images\n",
    "    images, raw_images = load_images(image_paths)\n",
    "    images = torch.stack(images).to(device)\n",
    "    \n",
    "    gcam = GradCAM(model=model)\n",
    "    probs, ids = gcam.forward(images)\n",
    "    ids_ = torch.LongTensor([[target_class]] * len(images)).to(device)\n",
    "    gcam.backward(ids=ids_)\n",
    "\n",
    "    for target_layer in target_layers:\n",
    "        print(\"Generating Grad-CAM @{}\".format(target_layer))\n",
    "\n",
    "        # Grad-CAM\n",
    "        regions = gcam.generate(target_layer=target_layer)\n",
    "\n",
    "        for j in range(len(images)):\n",
    "            print(\n",
    "                \"\\t#{}: {} ({:.5f})\".format(\n",
    "                    j, classes[target_class], float(probs[ids == target_class])\n",
    "                )\n",
    "            )\n",
    "\n",
    "            save_gradcam(\n",
    "                filename=osp.join(\n",
    "                    output_dir,\n",
    "                    \"{}-{}-gradcam-{}-{}.png\".format(\n",
    "                        j, \"resnet18\", target_layer, classes[target_class]\n",
    "                    ),\n",
    "                ),\n",
    "                gcam=regions[j, 0],\n",
    "                raw_image=raw_images[j],\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42a03c64-d779-4675-9772-c2e7d52bc7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: CPU\n",
      "Images:\n",
      "\t#0: ./data/images/tiger_shark.jpeg\n",
      "Generating Grad-CAM @relu\n",
      "\t#0: tiger_shark (0.97557)\n",
      "Generating Grad-CAM @layer1\n",
      "\t#0: tiger_shark (0.97557)\n",
      "Generating Grad-CAM @layer2\n",
      "\t#0: tiger_shark (0.97557)\n",
      "Generating Grad-CAM @layer3\n",
      "\t#0: tiger_shark (0.97557)\n",
      "Generating Grad-CAM @layer4\n",
      "\t#0: tiger_shark (0.97557)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/pm6_lt111h92v74z5vt3_vhh0000gn/T/ipykernel_95377/2648377605.py:65: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  gcam = (cmap.astype(np.float) + raw_image.astype(np.float)) / 2\n"
     ]
    }
   ],
   "source": [
    "LayersDemo([\"./data/images/tiger_shark.jpeg\"], 3, \"./data/outputs\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd658cc1-d91d-4b57-b2cd-ec19573699ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from explain_cnn.validators.base_validator import BaseValidator\n",
    "\n",
    "\n",
    "class BaseExplainer:\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        This class creates base of explainer module which works\n",
    "        as the fundamental module to work with this package.\n",
    "        \n",
    "        Args:\n",
    "            model: `torch.nn.Module` object containing the PyTorch model\n",
    "            device: `str` containing the type of device to execute the code\n",
    "        \"\"\"\n",
    "        \n",
    "        self.validator = BaseValidator()\n",
    "        self.validator.assert_type(model, nn.Module)\n",
    "        self.validator.assert_type(device, str)\n",
    "        \n",
    "        self.model = model\n",
    "        self.device = self.get_device(device)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "    \n",
    "    def get_device(self, device):\n",
    "        \"\"\"\n",
    "        Returns `torch.device` Object.\n",
    "        \n",
    "        Args:\n",
    "            device: `str` containing the type of device to execute the code\n",
    "        \n",
    "        Returns: `torch.device` object to allocate all tensors related to model\n",
    "        \"\"\"\n",
    "        return torch.device(device)\n",
    "    \n",
    "    def get_reverse_class_map(self, class_map):\n",
    "        \"\"\"\n",
    "        Generates reverse class mapping from index to label map.\n",
    "        \n",
    "        Args:\n",
    "            class_map: `dict` containing mapping from class index to label\n",
    "        \n",
    "        Returns: reverse class mapping from label to index\n",
    "        \"\"\"\n",
    "        reverse_class_mapping = {}\n",
    "        for index, label in class_map.items():\n",
    "            reverse_class_mapping[label] = index\n",
    "        return reverse_class_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82ed0731-687e-422f-9085-298319c48ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.BaseExplainer at 0x169a78a60>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BaseExplainer(models.resnet18(), \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15929720-796e-4601-a835-1fb9fbe01278",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNExplainer(BaseExplainer):\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 class_map,\n",
    "                 device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        This class creates base of explainer module which works\n",
    "        as the fundamental module to work with this package.\n",
    "        \n",
    "        Args:\n",
    "            model: `torch.nn.Module` object containing the PyTorch model\n",
    "            class_map: `dict` containing mapping from class index to label\n",
    "            device: `str` containing the type of device to execute the code\n",
    "        \"\"\"\n",
    "        \n",
    "        super(CNNExplainer, self).__init__(model, device)\n",
    "        \n",
    "        self.class_map = class_map\n",
    "        self.reverse_class_map = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn_xai",
   "language": "python",
   "name": "cnn_xai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
